{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HxQ-yMVz9tJG"
      },
      "outputs": [],
      "source": [
        "#imports: do not change them\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torchvision\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import random_split\n",
        "from torch.autograd import Variable\n",
        "import cv2\n",
        "import torchvision.utils as utils\n",
        "\n",
        "## Please DONOT remove these lines.\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1: MultiHead-Attention\n",
        "Given a set of query, key, value vectors, calculate the attention value using multihead attention.\n",
        "\n",
        "choose the closest option to the maximum value of the attention vector\n",
        "\n",
        "\n",
        "1.   0.5590\n",
        "2.   0.7851\n",
        "3.   0.2312\n",
        "4.   0.9452\n",
        "\n",
        "Answer (1)"
      ],
      "metadata": {
        "id": "_kvz_VZr3Otj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Please DO NOT change these values.\n",
        "#This is the config that will be used for implementing the Multi-head attention\n",
        "config = {\n",
        "    'QI_DIM': 64,\n",
        "    'VI_DIM': 64,\n",
        "    'KI_DIM': 64,\n",
        "    'QO_DIM': 32,\n",
        "    'VO_DIM': 32,\n",
        "    'OP_DIM': 32,\n",
        "    'NUM_HEADS': 8,\n",
        "}"
      ],
      "metadata": {
        "id": "g9mwliHI_1BC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A class for implementing the Multi-head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        ### YOUR CODE STARTS HERE ###\n",
        "\n",
        "        #set the op dimension, number of heads based on the value given in config\n",
        "        self.op_dim = config['OP_DIM']\n",
        "        self.n_heads = config['NUM_HEADS']\n",
        "\n",
        "        #set the query dimension as QO_DIM from the config and value dimension as VO_DIM\n",
        "        self.query_dim = config['QO_DIM']\n",
        "        self.value_dim = config['VO_DIM']\n",
        "\n",
        "        #set the respective dimensions\n",
        "        self.QTrans = nn.Linear(config['QI_DIM'], config['QO_DIM'])\n",
        "        self.KTrans = nn.Linear(config['KI_DIM'], config['QO_DIM'])\n",
        "        self.VTrans = nn.Linear(config['VI_DIM'], config['VO_DIM'])\n",
        "        self.OTrans = nn.Linear(config['VO_DIM'], config['OP_DIM'])\n",
        "\n",
        "        #set the scaling factor as follows:\n",
        "        #scale = sqrt(QO_DIM/NUM_HEADS)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.query_dim // self.n_heads]))\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, q_inp, k_inp, v_inp, ret_atn=False):\n",
        "\n",
        "        #making sure that shapes are similar across dim=0\n",
        "        assert q_inp.shape[0] == k_inp.shape[0]\n",
        "\n",
        "        #setting batch size\n",
        "        batch_size = q_inp.shape[0]\n",
        "        seq_length = q_inp.shape[1]\n",
        "\n",
        "        ### YOUR CODE STARTS HERE ###\n",
        "        #pass q_inp, k_inp, v_inp through the linear layers defined in the\n",
        "        #init function\n",
        "\n",
        "        Q = self.QTrans(q_inp)\n",
        "        K = self.KTrans(k_inp)\n",
        "        V = self.VTrans(v_inp)\n",
        "\n",
        "        #reshape the Q, K, V tensors to (batch_size, num_heads, -1, self.query_dim/num_heads)\n",
        "        #hint: you can also do the same by the view() of pytorch\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.query_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.query_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.value_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "\n",
        "        # define energy as matrix product of Q and K divided by self.scale value\n",
        "        #hint: use torch.matmul() for this and do not forget to reshape K vector as (batch_size, num_heads, self.query_dim/num_heads, -1)\n",
        "        #this should be done in order to carry out the matrix multiplication\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "        #define attention as softmax of energy vector across the last dimension\n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "        #multiplying the attention with value tensor\n",
        "        attended = torch.matmul(attention, V).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        #passing the attended tensor through OTrans linear layer\n",
        "        out = self.OTrans(attended.view(batch_size, seq_length, -1))\n",
        "\n",
        "        if ret_atn:\n",
        "            return out, attention\n",
        "\n",
        "        else:\n",
        "            return out"
      ],
      "metadata": {
        "id": "7S-NdiLS-1Q4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Please DONOT remove these lines.\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)\n",
        "\n",
        "q = torch.rand(4, 2, 64)\n",
        "k = torch.rand(4, 2, 64)\n",
        "v = torch.rand(4, 2, 64)\n",
        "## Please DONOT remove these lines.\n",
        "\n",
        "### YOUR CODE STARTS HERE ###\n",
        "\n",
        "#initialise an instance of MultiHeadAttention class\n",
        "d = MultiHeadAttention(config)\n",
        "\n",
        "#pass q, k, v values through the instance\n",
        "x, y = d(q, k, v, ret_atn = True)\n",
        "\n",
        "#print the max value of the attention and report your answer\n",
        "torch.max(y)\n",
        "\n",
        "### YOUR CODE ENDS HERE ###\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7oFAgh1AiLY",
        "outputId": "6786f2ff-e27c-49bd-ade6-b03afc18ba47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5590, grad_fn=<MaxBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2: Learning to pay attention\n",
        "For questions 2 & 3, we will implement the paper \"Learning to pay Attention\", published at ICLR2018. The paper focuses on Trainable Soft Visual Attention in CNNs for image classification task. Follow the steps given in the upcoming code blocks to answer question 2.\n",
        "\n",
        "What is the range of the mean value obtained?\n",
        "\n",
        "\n",
        "1.   0.01 - 0.1\n",
        "2.   0.1 - 0.3\n",
        "3.   0.5 - 0.8\n",
        "4.   1 - 1.5\n",
        "\n",
        "Answer (2)"
      ],
      "metadata": {
        "id": "akByJVIk9zC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a block that has conv->batchnorm->relu\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_conv, pool=False):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        features = [in_features] + [out_features for i in range(num_conv)]\n",
        "\n",
        "        #appending all the operations in the list and then converting them to sequential set of operations\n",
        "        layers = []\n",
        "        for i in range(len(features)-1):\n",
        "            layers.append(nn.Conv2d(in_channels=features[i], out_channels=features[i+1], kernel_size=3, padding=1, bias=True))\n",
        "            layers.append(nn.BatchNorm2d(num_features=features[i+1], affine=True, track_running_stats=True))\n",
        "            layers.append(nn.ReLU())\n",
        "            if pool:\n",
        "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
        "        self.op = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "#defining a projector block which is nothing but a conv layer that is used to transform the channel dimension\n",
        "class ProjectorBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(ProjectorBlock, self).__init__()\n",
        "        self.op = nn.Conv2d(in_channels=in_features, out_channels=out_features, kernel_size=1, padding=0, bias=False)\n",
        "    def forward(self, inputs):\n",
        "        return self.op(inputs)\n",
        "\n",
        "\n",
        "#defining a linear attention block as mentioned in the paper\n",
        "class LinearAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_features, normalize_attn=True):\n",
        "        super(LinearAttentionBlock, self).__init__()\n",
        "\n",
        "        ### YOUR CODE STARTS HERE ###\n",
        "        #set self.mormalize_attn as the normalize_attn argument\n",
        "        self.normalize_attn = normalize_attn\n",
        "\n",
        "        #define self.op as a conv layer with in_channels=in_features, out_channels=1, kernel_size=1, padding=0 and with bias=False\n",
        "        self.op = nn.Conv2d(in_channels=in_features, out_channels=1, kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "\n",
        "    def forward(self, l, g):\n",
        "        ### YOUR CODE STARTS HERE ###\n",
        "\n",
        "        #set N, C, W, H using the size of the 'l' tensor\n",
        "        #where N = number of batches, C = channels, H = height of the tensor, W = width of the tensor\n",
        "        N, C, W, H = l.size()\n",
        "\n",
        "        #add l and g as mentioned in the paper and pass it through the self.op layer\n",
        "        c = self.op(l+g) # batch_sizex1xWxH\n",
        "\n",
        "        #pass the output of the self.op layer through sigmoid activation function\n",
        "        a = torch.sigmoid(c)\n",
        "\n",
        "        #make the size of a same as l\n",
        "        #hint: you can do this by the tensor.expand_as() method of PyTorch\n",
        "\n",
        "        #multiple a and l using torch.mul()\n",
        "        g = torch.mul(a.expand_as(l), l)\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "        #using adaptive avg pooling as the final operation\n",
        "        g = F.adaptive_avg_pool2d(g, (1,1)).view(N,C)\n",
        "\n",
        "        return c.view(N,1,W,H), g\n"
      ],
      "metadata": {
        "id": "7mSxSxjnAlwO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Please DONOT remove/change these lines.\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)\n",
        "att = LinearAttentionBlock(in_features=16, normalize_attn=True)\n",
        "\n",
        "l = torch.rand(4, 16, 64, 64)\n",
        "g = torch.rand(4, 16, 64, 64)\n",
        "## Please DONOT remove/change these lines.\n",
        "\n",
        "\n",
        "### YOUR CODE STARTS HERE ###\n",
        "#pass the l and g vectors through the attention instance\n",
        "d, f = att(l, g)\n",
        "#take the mean of all the elements present in the output 'g' vector and mark your answer for question 2\n",
        "#hint: use torch.mean()\n",
        "val = torch.mean(f)\n",
        "print(val)\n",
        "### YOUR CODE ENDS HERE ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOpojpEbns-B",
        "outputId": "9591ddb5-8a6e-461d-f17c-d45260394f94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1692, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3: Training the Attention VGGNet\n",
        "Follow the steps in the below given code blocks to build and train the Attention VGGNet model on CIFAR100 dataset.\n",
        "\n",
        "What is the range of the test set accuracy obtained after training the model for 5 epochs?\n",
        "\n",
        "\n",
        "1.   0 - 2%\n",
        "2.   2 - 10%\n",
        "3.   40 - 50%\n",
        "4.   25 - 30%\n",
        "\n",
        "Answer (2)"
      ],
      "metadata": {
        "id": "xGq-e0tW_Cyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#building a VGGNet integrated with the attention implemented in the above code block\n",
        "class AttnVGG_before(nn.Module):\n",
        "    def __init__(self, im_size, num_classes, attention=True, normalize_attn=True):\n",
        "        super(AttnVGG_before, self).__init__()\n",
        "        self.attention = attention\n",
        "        # defining conv blocks with certain choices of input and output features\n",
        "        #this set of choices is taking from the VGG style model\n",
        "        self.conv_block1 = ConvBlock(3, 64, 2)\n",
        "        self.conv_block2 = ConvBlock(64, 128, 2)\n",
        "        self.conv_block3 = ConvBlock(128, 256, 3)\n",
        "        self.conv_block4 = ConvBlock(256, 512, 3)\n",
        "        self.conv_block5 = ConvBlock(512, 512, 3)\n",
        "        self.conv_block6 = ConvBlock(512, 512, 2, pool=True)\n",
        "        self.dense = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=int(im_size/32), padding=0, bias=True)\n",
        "\n",
        "        # defining the projector layers and the multiple attention blocks as mentioned in the paper\n",
        "        if self.attention:\n",
        "            self.projector = ProjectorBlock(256, 512)\n",
        "            self.attn1 = LinearAttentionBlock(in_features=512, normalize_attn=normalize_attn)\n",
        "            self.attn2 = LinearAttentionBlock(in_features=512, normalize_attn=normalize_attn)\n",
        "            self.attn3 = LinearAttentionBlock(in_features=512, normalize_attn=normalize_attn)\n",
        "\n",
        "        # final classification layer\n",
        "        if self.attention:\n",
        "            self.classify = nn.Linear(in_features=512*3, out_features=num_classes, bias=True)\n",
        "        else:\n",
        "            self.classify = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ### YOUR CODE STARTS HERE ###\n",
        "\n",
        "        # passing the input through all the defined layers\n",
        "        #pass x through conv_block1 and collect the output in variable name x\n",
        "        x = self.conv_block1(x)\n",
        "\n",
        "        #pass x through conv_block2 and collect the output in variable name x\n",
        "        x = self.conv_block2(x)\n",
        "\n",
        "        #pass x through conv_block3 and collect the output in variable name l1\n",
        "        l1 = self.conv_block3(x)\n",
        "\n",
        "        #pass l1 through max pool 2d and collect the output in variable name x\n",
        "        x = F.max_pool2d(l1, kernel_size=2, stride=2, padding=0) # /2\n",
        "\n",
        "        #pass x through conv_block4 and collect the output in av ariable called l2\n",
        "        l2 = self.conv_block4(x) # /2\n",
        "\n",
        "        #pass l2 through max pool 2d and collect the output in a variable called x\n",
        "        x = F.max_pool2d(l2, kernel_size=2, stride=2, padding=0) # /4\n",
        "\n",
        "        #pass x through conv_block5 and collect the output in a variable called l3\n",
        "        l3 = self.conv_block5(x) # /4\n",
        "\n",
        "       #pass l3 through max pool 2d and collect the output in variable name x\n",
        "        x = F.max_pool2d(l3, kernel_size=2, stride=2, padding=0) # /8\n",
        "\n",
        "        #pass x through conv_block6 and collect the output in a variable called x\n",
        "        x = self.conv_block6(x) # /32\n",
        "\n",
        "        #pass x through self.dense and collect the output in a variable called g\n",
        "        g = self.dense(x) # batch_sizex512x1x1\n",
        "\n",
        "        # attention calculations are carried out according to the paper\n",
        "        if self.attention:\n",
        "\n",
        "            #pass l1 through self.projector and collect the output in variable t\n",
        "            #pass g and t through self.attn1 and collect the output in variable names c1 and g1 respectively\n",
        "            c1, g1 = self.attn1(self.projector(l1), g)\n",
        "\n",
        "            #pass l2 and g through self.attn2 and collect the output in variable names c2 and g2 respectively\n",
        "            c2, g2 = self.attn2(l2, g)\n",
        "\n",
        "            #pass l3 and g through self.attn3 and collect the output in variable names c3 and g3 respectively\n",
        "            c3, g3 = self.attn3(l3, g)\n",
        "\n",
        "            #concat g1, g2 and g3 across dim=1 and collect the output in a variable called g\n",
        "            #hint:use torch.cat()\n",
        "            g = torch.cat((g1,g2,g3), dim=1)\n",
        "\n",
        "            # pass g through self.classify layer and collect the output in a variable called x\n",
        "            x = self.classify(g)\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "        #if attention is set to false\n",
        "        else:\n",
        "            c1, c2, c3 = None, None, None\n",
        "            x = self.classify(torch.squeeze(g))\n",
        "\n",
        "        return [x, c1, c2, c3]"
      ],
      "metadata": {
        "id": "1TejCUDeAiXO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining arg parse and running it\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size\")\n",
        "parser.add_argument(\"--epochs\", type=int, default=5, help=\"number of epochs\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.1, help=\"initial learning rate\")\n",
        "parser.add_argument(\"-f\", \"--file\", required=False)\n",
        "\n",
        "\n",
        "opt = parser.parse_args()"
      ],
      "metadata": {
        "id": "Ozg7UV1B4qqy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the main function for training\n",
        "def main():\n",
        "    print('\\nloading the dataset ...\\n')\n",
        "\n",
        "    #setting the image size to 32 as we will be using CIFAR100 dataset\n",
        "    im_size = 32\n",
        "\n",
        "    #defining train transforms\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "    ])\n",
        "\n",
        "    #defining the trainset from the torchvision library\n",
        "    trainset = torchvision.datasets.CIFAR100(root='CIFAR100_data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "    #we will be training on a random 5000 image subset of the training data to save time\n",
        "    sub_train_size = 5000\n",
        "    train_size = len(trainset) - sub_train_size\n",
        "    extra_ds, train_ds = random_split(trainset, [train_size, sub_train_size], generator=torch.Generator().manual_seed(30))\n",
        "\n",
        "    #defining the dataloader\n",
        "    trainloader = torch.utils.data.DataLoader(train_ds, batch_size=opt.batch_size, shuffle=False, num_workers=0, worker_init_fn = lambda id: np.random.seed(id))\n",
        "    # testset = torchvision.datasets.CIFAR100(root='CIFAR100_data', train=False, download=False, transform=transform_test)\n",
        "    # testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=0, worker_init_fn = lambda id: np.random.seed(id))\n",
        "    print('done')\n",
        "\n",
        "    print('\\npay attention before maxpooling layers...\\n')\n",
        "    #calling the model that needs to be trained\n",
        "    net = AttnVGG_before(im_size=im_size, num_classes=100,\n",
        "        attention=True, normalize_attn=True)\n",
        "\n",
        "    #setting the loss function to CE loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print('done')\n",
        "\n",
        "    #setting the device to GPU and transfering model and loss function tot he device\n",
        "    print('\\nmoving to GPU ...\\n')\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = net.to(device)\n",
        "    criterion.to(device)\n",
        "    print('done')\n",
        "\n",
        "    # defining the optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=opt.lr, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "    # training loop starts here\n",
        "    print('\\nstart training ...\\n')\n",
        "    step = 0\n",
        "\n",
        "    #looping over the total number of epochs\n",
        "    for epoch in range(opt.epochs):\n",
        "\n",
        "        # adjusting the learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\"\\nepoch %d learning rate %f\\n\" % (epoch, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "        # looping for every epoch\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            model.train()\n",
        "            model.zero_grad()\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = data\n",
        "\n",
        "            #transfering the data to device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # forward passing the inputs to the model\n",
        "            pred, __, __, __ = model(inputs)\n",
        "\n",
        "            # backwardpass for loss calculation\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # display results for every 3 iterations\n",
        "            if i % 3 == 0:\n",
        "                model.eval()\n",
        "                pred, __, __, __ = model(inputs)\n",
        "                predict = torch.argmax(pred, 1)\n",
        "                total = labels.size(0)\n",
        "                correct = torch.eq(predict, labels).sum().double().item()\n",
        "                accuracy = correct / total\n",
        "                print(\"[epoch %d][%d/%d] loss %.4f accuracy %.2f%%\"\n",
        "                    % (epoch, i, len(trainloader)-1, loss.item(), (100*accuracy)))\n",
        "            step += 1\n",
        "\n",
        "    #saving the weights of the model at the end of total number of epochs\n",
        "    torch.save(model.state_dict(), 'net.pth')"
      ],
      "metadata": {
        "id": "6G_4laZt3vph"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#please change the runtime type to GPU before running this cell\n",
        "#follow the instructions given in this blog for more details: https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xz929PM7Kan",
        "outputId": "6fcaa9fa-386f-445e-b6e5-b36c353be3a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "loading the dataset ...\n",
            "\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to CIFAR100_data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:02<00:00, 83838266.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting CIFAR100_data/cifar-100-python.tar.gz to CIFAR100_data\n",
            "done\n",
            "\n",
            "pay attention before maxpooling layers...\n",
            "\n",
            "done\n",
            "\n",
            "moving to GPU ...\n",
            "\n",
            "done\n",
            "\n",
            "start training ...\n",
            "\n",
            "\n",
            "epoch 0 learning rate 0.095000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 0][0/39] loss 4.6173 accuracy 1.56%\n",
            "[epoch 0][3/39] loss 4.6046 accuracy 1.56%\n",
            "[epoch 0][6/39] loss 4.6069 accuracy 0.78%\n",
            "[epoch 0][9/39] loss 4.5188 accuracy 0.00%\n",
            "[epoch 0][12/39] loss 4.4626 accuracy 0.78%\n",
            "[epoch 0][15/39] loss 4.5237 accuracy 4.69%\n",
            "[epoch 0][18/39] loss 4.5295 accuracy 0.78%\n",
            "[epoch 0][21/39] loss 4.4085 accuracy 1.56%\n",
            "[epoch 0][24/39] loss 4.3856 accuracy 0.78%\n",
            "[epoch 0][27/39] loss 4.5155 accuracy 2.34%\n",
            "[epoch 0][30/39] loss 4.3671 accuracy 2.34%\n",
            "[epoch 0][33/39] loss 4.4289 accuracy 1.56%\n",
            "[epoch 0][36/39] loss 4.4234 accuracy 2.34%\n",
            "[epoch 0][39/39] loss 4.1413 accuracy 25.00%\n",
            "\n",
            "epoch 1 learning rate 0.090250\n",
            "\n",
            "[epoch 1][0/39] loss 4.4427 accuracy 3.12%\n",
            "[epoch 1][3/39] loss 4.4668 accuracy 1.56%\n",
            "[epoch 1][6/39] loss 4.3477 accuracy 5.47%\n",
            "[epoch 1][9/39] loss 4.1920 accuracy 3.91%\n",
            "[epoch 1][12/39] loss 4.3544 accuracy 5.47%\n",
            "[epoch 1][15/39] loss 4.3324 accuracy 4.69%\n",
            "[epoch 1][18/39] loss 4.2770 accuracy 4.69%\n",
            "[epoch 1][21/39] loss 4.2816 accuracy 3.12%\n",
            "[epoch 1][24/39] loss 4.1751 accuracy 6.25%\n",
            "[epoch 1][27/39] loss 4.3243 accuracy 5.47%\n",
            "[epoch 1][30/39] loss 4.2200 accuracy 8.59%\n",
            "[epoch 1][33/39] loss 4.0849 accuracy 6.25%\n",
            "[epoch 1][36/39] loss 4.2234 accuracy 6.25%\n",
            "[epoch 1][39/39] loss 3.6211 accuracy 37.50%\n",
            "\n",
            "epoch 2 learning rate 0.085737\n",
            "\n",
            "[epoch 2][0/39] loss 4.3187 accuracy 5.47%\n",
            "[epoch 2][3/39] loss 4.1632 accuracy 6.25%\n",
            "[epoch 2][6/39] loss 4.1757 accuracy 5.47%\n",
            "[epoch 2][9/39] loss 4.0809 accuracy 7.03%\n",
            "[epoch 2][12/39] loss 4.2537 accuracy 5.47%\n",
            "[epoch 2][15/39] loss 4.1509 accuracy 6.25%\n",
            "[epoch 2][18/39] loss 4.2002 accuracy 6.25%\n",
            "[epoch 2][21/39] loss 4.0691 accuracy 8.59%\n",
            "[epoch 2][24/39] loss 4.0750 accuracy 7.81%\n",
            "[epoch 2][27/39] loss 4.2301 accuracy 3.91%\n",
            "[epoch 2][30/39] loss 4.0770 accuracy 11.72%\n",
            "[epoch 2][33/39] loss 3.9664 accuracy 7.81%\n",
            "[epoch 2][36/39] loss 4.0558 accuracy 7.03%\n",
            "[epoch 2][39/39] loss 2.6662 accuracy 50.00%\n",
            "\n",
            "epoch 3 learning rate 0.081451\n",
            "\n",
            "[epoch 3][0/39] loss 4.0972 accuracy 7.81%\n",
            "[epoch 3][3/39] loss 4.0296 accuracy 5.47%\n",
            "[epoch 3][6/39] loss 4.1375 accuracy 2.34%\n",
            "[epoch 3][9/39] loss 4.0463 accuracy 5.47%\n",
            "[epoch 3][12/39] loss 4.1317 accuracy 7.03%\n",
            "[epoch 3][15/39] loss 4.1064 accuracy 6.25%\n",
            "[epoch 3][18/39] loss 4.1559 accuracy 7.03%\n",
            "[epoch 3][21/39] loss 4.0528 accuracy 6.25%\n",
            "[epoch 3][24/39] loss 4.0282 accuracy 8.59%\n",
            "[epoch 3][27/39] loss 4.1374 accuracy 9.38%\n",
            "[epoch 3][30/39] loss 4.0261 accuracy 7.81%\n",
            "[epoch 3][33/39] loss 3.8664 accuracy 10.16%\n",
            "[epoch 3][36/39] loss 4.0450 accuracy 4.69%\n",
            "[epoch 3][39/39] loss 2.8162 accuracy 12.50%\n",
            "\n",
            "epoch 4 learning rate 0.077378\n",
            "\n",
            "[epoch 4][0/39] loss 4.8856 accuracy 0.00%\n",
            "[epoch 4][3/39] loss 4.8695 accuracy 0.78%\n",
            "[epoch 4][6/39] loss 4.4158 accuracy 3.12%\n",
            "[epoch 4][9/39] loss 4.3029 accuracy 5.47%\n",
            "[epoch 4][12/39] loss 4.3265 accuracy 2.34%\n",
            "[epoch 4][15/39] loss 4.2492 accuracy 4.69%\n",
            "[epoch 4][18/39] loss 4.3712 accuracy 0.00%\n",
            "[epoch 4][21/39] loss 4.2053 accuracy 1.56%\n",
            "[epoch 4][24/39] loss 4.2001 accuracy 3.12%\n",
            "[epoch 4][27/39] loss 4.2154 accuracy 4.69%\n",
            "[epoch 4][30/39] loss 4.2845 accuracy 6.25%\n",
            "[epoch 4][33/39] loss 3.9894 accuracy 7.81%\n",
            "[epoch 4][36/39] loss 4.1731 accuracy 4.69%\n",
            "[epoch 4][39/39] loss 3.1808 accuracy 25.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "    ])\n",
        "testset = torchvision.datasets.CIFAR100(root='CIFAR100_data', train=False, download=False, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=0, worker_init_fn = lambda id: np.random.seed(id))\n",
        "model = AttnVGG_before(im_size=32, num_classes=100, attention=True, normalize_attn=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load('/content/net.pth', map_location=torch.device(\"cuda\")))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "        images_test, labels_test = data\n",
        "        images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
        "        pred_test, __, __, __ = model(images_test)\n",
        "        predict = torch.argmax(pred_test, 1)\n",
        "        total += labels_test.size(0)\n",
        "        correct += torch.eq(predict, labels_test).sum().double().item()\n",
        "    print(\"\\nAccuracy on test data: %.2f%%\\n\" % (100*correct/total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbvpZ-8A9vbF",
        "outputId": "e8ff5684-9b58-4fd0-a8f2-e07776b7ce02"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy on test data: 6.23%\n",
            "\n"
          ]
        }
      ]
    }
  ]
}