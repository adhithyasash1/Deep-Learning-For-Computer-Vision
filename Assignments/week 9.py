# -*- coding: utf-8 -*-
"""week 9 sol.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13xiH6dUtt0igWqLggUxW81sc7EvT8P_X

#### **Welcome to Assignment 9 on Deep Learning for Computer Vision.**
This assignment is based on the content you learned in Week-9 of course.

#### **Instructions**
1. Use Python 3.x to run this notebook
2. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.
you should not change anything else in the code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.
3. Read documentation of each function carefully.
4. All the Best!

# Cycle-GAN

In this assignment we will implement a Cycle-GAN. Please refer to the lecture and the following paper: https://arxiv.org/abs/1703.10593 for an understanding of how Cycle-GAN works.
"""

# Commented out IPython magic to ensure Python compatibility.
import math
import torch
import torch.nn as nn
from torch.nn import init
import torchvision
import random
import os
import numpy as np
import torchvision.transforms as T
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.data import sampler
import torchvision.datasets as dset
import itertools

import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

# %matplotlib inline
plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

# Please do not modify anything in this cell

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)

set_seed(0)

# Please do not modify anything in this cell

def show_images(images):
    images = torch.reshape(images, [images.shape[0], -1])  # images reshape to (batch_size, D)
    sqrtn = int(math.ceil(math.sqrt(images.shape[0])))
    sqrtimg = int(math.ceil(math.sqrt(images.shape[1])))

    fig = plt.figure(figsize=(sqrtn, sqrtn))
    gs = gridspec.GridSpec(sqrtn, sqrtn)
    gs.update(wspace=0.05, hspace=0.05)

    for i, img in enumerate(images):
        ax = plt.subplot(gs[i])
        plt.axis('off')
        ax.set_xticklabels([])
        ax.set_yticklabels([])
        ax.set_aspect('equal')
        plt.imshow(img.reshape([sqrtimg,sqrtimg]))
    return

# Please do not modify anything in this cell

dtype = torch.float

# set device

device = torch.device("cuda:0")

"""## Preparing the data

Cycle-GAN is used to translate data from a source domain to a target domain using unpaired samples from both domains. For this implementation we will consider the problem of translating from the usual MNIST images(bright digits with dark background) to their negatives(dark digits in bright background). Although this is a simple translation that can be achieved by pixed value inversion, it will highlight the working principle of Cycle-GAN. We will prepare the target domain data by manually applying the transformation to the regular MNIST data. Note that the generated data does not have a direct mapping between the images in the source domain and their counter parts in the target domain.
"""

# Please do not modify anything in this cell

# let us load the input images from the dataset and visualize some images!

batch_size = 128

print('download MNIST if not exist')

transform_A = T.Compose([
        T.ToTensor(), # convert the image to a pytorch tensor
        ])

transform_B = T.Compose([
        T.ToTensor(), # convert the image to a pytorch tensor
        lambda x: T.functional.invert(x) # convert images to their negatives
        ])


# source domain data denoted by domain A
mnist_A_train = dset.MNIST('./data', train=True, download=True,
                       transform=transform_A)

loader_A_train = DataLoader(mnist_A_train, batch_size=batch_size,
                          shuffle=True, drop_last=True, num_workers=2)


# target domain data denoted by doamin B
mnist_B_train = dset.MNIST('./data', train=True, download=True,
                       transform=transform_B)

loader_B_train = DataLoader(mnist_B_train, batch_size=batch_size,
                          shuffle=True, drop_last=True, num_workers=2)


# Get the next batch of images from the iterators
imgs_A, _ = next(iter(loader_A_train))
imgs_B, _ = next(iter(loader_B_train))

imgs_A = imgs_A.view(batch_size, 784)
imgs_B = imgs_B.view(batch_size, 784)

show_images(imgs_A)
show_images(imgs_B)

"""## Generator and Discriminator

Our Cycle-GAN implementation will contain 2 instances of generators and 1 instance of a discriminator with the following architecture.

### Discriminator

The architecture is:
 * Flatten (Hint: nn.Flatten())
 * Fully connected layer with input size 784 and output size 256
 * LeakyReLU with alpha 0.01
 * Fully connected layer with input_size 256 and output size 256
 * LeakyReLU with alpha 0.01
 * Fully connected layer with input size 256 and output size 1 (no activation at output)
  
The output of the discriminator is of shape `[batch_size, 1]`, and contains real numbers corresponding to the scores that each of the `batch_size` inputs is a real image.

### Generator

The architecture is:
 * Fully connected layer from 784 to 128
 * `ReLU`
 * Fully connected layer from 128 to 784
 * `TanH` (to clip the image to be in the range of [-1,1])

We will be creating two instances of this generator to go from domain A to B and from B to A respectively. The output of the generators are of shape `[batch_size, 784]`.
"""

def discriminator():

  #### YOUR CODE STARTS HERE ####

  # Implement a PyTorch nn.Sequential model for the discriminator architecture given above

  model = nn.Sequential(
        nn.Flatten(),
        nn.Linear(784, 256),
        nn.LeakyReLU(0.01),
        nn.Linear(256, 256),
        nn.LeakyReLU(0.01),
        nn.Linear(256, 1)
    )

  #### YOUR CODE ENDS HERE ####

  return model


def generator():

  #### YOUR CODE STARTS HERE ####

  # Implement a PyTorch nn.Sequential model for the generator architecture given above

  model = nn.Sequential(
        nn.Linear(784, 128),
        nn.ReLU(),
        nn.Linear(128, 784),
        nn.Tanh()
    )
  #### YOUR CODE ENDS HERE ####

  return model

"""## Loss functions

Cycle-GAN involves the generator, discriminator and the cyclic loss. Please refer to the lecture for detailed explanations.

The generator loss is:
$$\ell_G  =  -\mathbb{E}_{z \sim p(z)}\left[\log D(G(z))\right]$$
The discriminator loss is:
$$ \ell_D = -\mathbb{E}_{x \sim p_\text{data}}\left[\log D(x)\right] - \mathbb{E}_{z \sim p(z)}\left[\log \left(1-D(G(z))\right)\right]$$
The cyclic loss is:
$$ \ell_C = -\mathbb{E}_{x \sim p_\text{data}}\left||G_{YX}(G_{XY}(x)- x\right||$$
We will be *minimizing* these losses. We have already implemented the functions to compute generator and discriminator losses. Please go trhough them carefully. You need to write the function for cyclic loss.
"""

def discriminator_loss(logits_real, logits_fake):
  """
  Computes the discriminator loss described above.

  Inputs:
  - logits_real: PyTorch Tensor of shape (batch_size, 1) giving scores for the real data.
  - logits_fake: PyTorch Tensor of shape (batch_size, 1) giving scores for the fake data.

  Returns:
  - loss: PyTorch Tensor containing (scalar) the loss for the discriminator.
  """
  loss = None

  # For the discriminator (D), the true target (y = 1) corresponds to "real" images.
  # Thus, for the scores of real images, the target is always 1 (a vector).
  real_labels = torch.ones_like(logits_real, device=device)
  # Compute the BCE for the scores of the real images.
  # Note that the BCE itself uses the Expectation formula (in addition, an average is
  # taken throughout the losses, not a sum [as requested in this assignment]).
  real_loss = F.binary_cross_entropy_with_logits(logits_real, real_labels)

  # For D, the false target (y = 0) corresponds to "fake" images.
  # Thus, for the scores of fake images, the target is always 0 (a vector).
  fake_labels = torch.zeros_like(logits_fake, device=device)
  # As for the real scores, compute the BCE loss for the fake images.
  fake_loss = F.binary_cross_entropy_with_logits(logits_fake, fake_labels)

  # Sum "real" and "fake" losses.
  # That is, BCE has already taken into account the "negated equation" form,
  # the "log" (in the Expectation) and the "mean" (insetead on the "sum").
  loss = real_loss + fake_loss

  return loss

def generator_loss(logits_fake):
  """
  Computes the generator loss described above.

  Inputs:
  - logits_fake: PyTorch Tensor of shape (batch_size, 1) giving scores for the fake data.

  Returns:
  - loss: PyTorch Tensor containing (scalar) the loss for the generator.
  """
  loss = None

  # For the generator (G), the true target (y = 1) corresponds to "fake" images.
  # Thus, for the scores of fake images, the target is always 1 (a vector).
  fake_labels = torch.ones_like(logits_fake, device=device)
  # Compute the BCE for the scores of the fake images.
  fake_loss = F.binary_cross_entropy_with_logits(logits_fake, fake_labels)

  # The generator loss is "fake_loss".
  # That is, BCE has already taken into account the "negated equation" form,
  # the "log" (in the Expectation) and the "mean" (insetead on the "sum").
  loss = fake_loss

  return loss

def cycle_loss(original_input, recovered_input):

  """
  Computes the cyclic loss described above.

  Inputs:
  - original_input: PyTorch Tensor of shape (batch_size, 784) represented a batch of input images.
  - recovered_input: PyTorch PyTorch Tensor of shape (batch_size, 784) represented a batch of reconstructed images.

  Returns:
  - loss: PyTorch Tensor containing (scalar) the cyclic loss.
  """
  #### YOUR CODE STARTS HERE ####

  # Implement the cyclic loss as the mean pixel-wise absolute value of difference
  # between the original input and recovered input
  # Hint : F.l1_loss
  loss = F.l1_loss(original_input, recovered_input)

  #### YOUR CODE ENDS HERE ####

  return loss

# Please do not modify anything in this cell

# this function will be used to alternatively iterate through the batches in both domains
def alternate(*iters):
    for row in zip(*iters):
       for i in row:
           yield i

"""## Training

Complete the code below for the main training loop of the Cycle_GAN.
"""

def run_cycle_gan(D, G_A2B, G_B2A, D_solver, G_solver, C_solver, discriminator_loss, generator_loss, cycle_loss, show_every=250, batch_size=128, num_epochs=10):
  iter_count = 0
  for epoch in range(num_epochs):
    for x, _ in alternate(loader_A_train, loader_B_train):
      if len(x) != batch_size:
        continue
      if iter_count%2 == 0:
        real_data_A = x.view(-1, 784).to(device)
        real_data_A = 2*(real_data_A - 0.5)
        iter_count += 1
        continue

      real_data_B = x.view(-1, 784).to(device)
      real_data_B = 2*(real_data_B - 0.5)

      G_A2B.zero_grad()
      G_B2A.zero_grad()

      """
      Optimizing discriminator loss
      """

      D_solver.zero_grad()
      logits_real_B = D(real_data_B)
      fake_data_B = G_A2B(real_data_A)
      dis_logits_fake_B = D(fake_data_B)
      d_total_error = discriminator_loss(logits_real_B, dis_logits_fake_B)
      d_total_error.backward(retain_graph=True)  # Retain the graph for subsequent backward passes
      D_solver.step()

      """
      Optimizing cyclic loss (from A->B->A as well as B->A->B)
      """

      C_solver.zero_grad()
      recovered_A = G_B2A(fake_data_B)
      c_error = cycle_loss(real_data_A, recovered_A)

      fake_data_A = G_B2A(real_data_B)
      recovered_B = G_A2B(fake_data_A)
      c_error += cycle_loss(real_data_B, recovered_B)
      c_error.backward()
      C_solver.step()

      """
      Optimizing generator loss for the generator from A to B to fool the discriminator
      """

      G_solver.zero_grad()
      fake_data_B = G_A2B(real_data_A)
      gen_logits_fake_B = D(fake_data_B)
      g_error = generator_loss(gen_logits_fake_B)
      g_error.backward(retain_graph=True)  # Retain the graph for subsequent backward passes
      G_solver.step()

      if (((iter_count-1)/2) % show_every == 0):
        print('Iter: {}, D: {:.4}, G:{:.4}, C:{:.4}'.format((iter_count-1)/2,d_total_error.item(),g_error.item(), c_error.item()))

        print('\n Source')
        imgs_numpy = real_data_A.data.cpu()
        show_images(imgs_numpy[0:16])
        plt.show()

        print('Target')
        imgs_numpy = fake_data_B.data.cpu()
        show_images(imgs_numpy[0:16])
        plt.show()

        print()
      iter_count += 1

"""## Executing the Cycle-GAN

Now run the cell below to train the Cycle-GAN!

After every 250 iterations you should see a grid of images from domain A (regular MNIST) and a grid of the generated corresponding images in domain B (negative MNIST). Initially many digits in the target grid will differ from the corresponding images in the source grid but will be digits of similar shapes. Finally there should be a correct or near correct translation from the source domain to the target domain.
"""

# Please do not modify anything in this cell

set_seed(0)

# Make the discriminator
D = discriminator().to(device)

# Make the generators
G_A2B = generator().to(device)
G_B2A = generator().to(device)


# Create optimizers for the Discriminator and the Generators
D_solver = optim.Adam(D.parameters(), lr=1e-3, betas=(0.5, 0.999))
G_solver = optim.Adam(G_A2B.parameters(), lr=1e-3, betas=(0.5, 0.999))
C_solver = optim.Adam(itertools.chain(G_A2B.parameters(), G_B2A.parameters()), lr=1e-3, betas=(0.5, 0.999))

# Run it!
run_cycle_gan(D, G_A2B, G_B2A, D_solver, G_solver, C_solver, discriminator_loss, generator_loss, cycle_loss)

"""Now answer the following questions based on the results.

## Question 1

What is the range of final discriminator error displayed in the cell? (choose the closest answer)

1. 0-0.99
2. 1-1.99
3. 2-2.99
4. 3-3.99

## Question 2

What is the range of final generator error displayed in the cell? (choose the closest answer)

1. 0-0.99
2. 1-1.99
3. 2-2.99
4. 3-3.99

## Question 3

What is the range of final cyclic error displayed in the cell? (choose the closest answer)

1. 0-0.99
2. 1-1.99
3. 2-2.99
4. 3-3.99
"""

