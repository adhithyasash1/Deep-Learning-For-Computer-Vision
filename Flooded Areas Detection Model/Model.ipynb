{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UMTSJgr8Fi-"
      },
      "source": [
        "# Detailed Report on Flood Detection Model for Disaster Management\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "The objective of this project was to develop a computer vision model that can effectively segment images to identify areas affected by flooding. This task is crucial for aiding disaster management efforts by providing accurate information about the extent and location of flood-impacted regions. By distinguishing between flooded and non-flooded areas within an image, the model supports critical activities such as planning rescue operations, assessing damage, and prioritizing resources during flood events. Furthermore, the segmented data can be utilized for post-event analysis and future flood prediction and planning.\n",
        "\n",
        "## Approach and Methodology\n",
        "\n",
        "### Data Preparation\n",
        "\n",
        "The dataset provided consists of training and testing images along with their corresponding segmentation masks. These masks define the flooded areas within each image. Initial steps involved preprocessing the data to normalize the image sizes and pixel values, ensuring consistency across the dataset. Data augmentation techniques such as rotations, scaling, and horizontal flipping were employed to enhance model robustness and prevent overfitting.\n",
        "\n",
        "### Model Architecture: U-Net\n",
        "\n",
        "The U-Net architecture was chosen due to its proven efficacy in tasks requiring precise localization, such as medical image segmentation. This architecture is particularly suitable for segmenting small objects and detailed textures in images, which is analogous to identifying nuanced differences in flooded areas.\n",
        "\n",
        "#### Key Features of U-Net:\n",
        "- **Symmetric Structure:** The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization.\n",
        "- **Skip Connections:** These connections between layers of equal resolution in the contracting and expanding paths help the model retain important high-resolution features.\n",
        "\n",
        "### Training Process\n",
        "\n",
        "#### Loss Function:\n",
        "- **BCEWithLogitsLoss:** This loss function combines a sigmoid layer and the binary cross-entropy loss in one single class, which is particularly suitable for binary classification tasks like segmentation.\n",
        "\n",
        "#### Optimizer:\n",
        "- **AdamW Optimizer:** Known for its effectiveness in deep learning tasks, AdamW was chosen due to its ability to combine the benefits of Adam optimization with weight decay regulation, providing better control over learning.\n",
        "\n",
        "### Parameters:\n",
        "- **Epochs:** The model was trained over 10 epochs.\n",
        "- **Learning Rate:** Initially set at 0.001.\n",
        "- **Batch Size:** Determined by the computational limits of the training environment, aiming for a balance between speed and memory usage.\n",
        "\n",
        "## Results and Evaluation\n",
        "\n",
        "After training for 10 epochs, the model achieved an **Average IoU of 0.68** on the test set. This metric, which ranges from 0 to 1, measures the overlap between the predicted segmentation and the actual mask, with 1 representing perfect overlap and 0 representing no overlap.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The U-Net model demonstrated a promising ability to segment flooded areas from aerial images, achieving a moderate IoU score. This performance indicates that the model can effectively contribute to disaster management efforts by providing reliable data for assessing and responding to flood situations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUE9bJb_zHXV"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVR3V-A1zLHc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqePGnIrzi8E"
      },
      "source": [
        "# Model Arch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMlwLgTwbXTO"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def CBR(in_ch, out_ch, kernel_size=3, stride=1, padding=1, dropout_rate=0.1):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            )\n",
        "\n",
        "        def encoder(in_ch, out_ch):\n",
        "            return nn.Sequential(\n",
        "                CBR(in_ch, out_ch),\n",
        "                CBR(out_ch, out_ch),\n",
        "                nn.MaxPool2d(2)\n",
        "            )\n",
        "\n",
        "        def decoder(in_ch, out_ch, kernel_size=2, stride=2, padding=0):\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_ch, out_ch, kernel_size, stride, padding),\n",
        "                CBR(out_ch, out_ch),\n",
        "                CBR(out_ch, out_ch),\n",
        "            )\n",
        "\n",
        "        self.enc1 = encoder(in_channels, 64)\n",
        "        self.enc2 = encoder(64, 128)\n",
        "        self.enc3 = encoder(128, 256)\n",
        "        self.enc4 = encoder(256, 512)\n",
        "\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            CBR(512, 1024),\n",
        "            CBR(1024, 1024),\n",
        "            CBR(1024, 1024),  # Added an extra CBR layer to match the output channels\n",
        "        )\n",
        "\n",
        "        self.dec1 = decoder(1536, 1024)  # bottleneck (1024) + enc4 (512) => 1536\n",
        "        self.dec2 = decoder(1024 + 256, 512)\n",
        "        self.dec3 = decoder(512 + 128, 256)\n",
        "        self.dec4 = decoder(256 + 64, 128)\n",
        "\n",
        "        self.final = nn.Conv2d(128, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(enc1)\n",
        "        enc3 = self.enc3(enc2)\n",
        "        enc4 = self.enc4(enc3)\n",
        "\n",
        "        bottleneck = self.bottleneck(enc4)\n",
        "\n",
        "        def crop_and_concat(up_tensor, down_tensor):\n",
        "            _, _, H, W = up_tensor.shape\n",
        "            down_tensor_cropped = torchvision.transforms.CenterCrop([H, W])(down_tensor)\n",
        "            return torch.cat([up_tensor, down_tensor_cropped], 1)\n",
        "\n",
        "        dec1 = self.dec1(crop_and_concat(bottleneck, enc4))\n",
        "        dec2 = self.dec2(crop_and_concat(dec1, enc3))\n",
        "        dec3 = self.dec3(crop_and_concat(dec2, enc2))\n",
        "        dec4 = self.dec4(crop_and_concat(dec3, enc1))\n",
        "\n",
        "        return self.final(dec4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlqzYX5Jzo9_"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkW3V04Cchqc"
      },
      "outputs": [],
      "source": [
        "class FloodDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, mask_dir, image_transform=None, mask_transform=None):\n",
        "        self.img_labels = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        mask_name = os.path.join(self.mask_dir, self.img_labels.iloc[idx, 1])\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        mask = Image.open(mask_name).convert(\"L\")\n",
        "\n",
        "        if self.image_transform:\n",
        "            image = self.image_transform(image)\n",
        "\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArDbNwrCclPr"
      },
      "outputs": [],
      "source": [
        "data_augmentation = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "])\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = FloodDataset(\n",
        "    csv_file='/content/drive/MyDrive/DLCV Project/Flood-Detection-Project/train-metadata.csv',\n",
        "    img_dir='/content/drive/MyDrive/DLCV Project/Flood-Detection-Project/Image/Train-images',\n",
        "    mask_dir='/content/drive/MyDrive/DLCV Project/Flood-Detection-Project/Mask/Train-Masks',\n",
        "    image_transform=image_transform,\n",
        "    mask_transform=mask_transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "test_dataset = FloodDataset(\n",
        "    csv_file='/content/drive/MyDrive/DLCV Project/Flood-Detection-Project/test-metadata.csv',\n",
        "    img_dir='/content/drive/MyDrive/DLCV Project/Flood-Detection-Project/Image/Test-images',\n",
        "    mask_dir='/content/drive/MyDrive/DLCV Project/Flood-Detection-Project/Mask/Test-Masks',\n",
        "    image_transform=image_transform,\n",
        "    mask_transform=mask_transform\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO34H9Bezy6F"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfZob3j-covZ",
        "outputId": "a7f40d58-5e02-4eb8-9daf-39521021eefd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.022475264966487885\n",
            "Epoch 2, Loss: 0.027996350079774857\n",
            "Epoch 3, Loss: 0.01992657780647278\n",
            "Epoch 4, Loss: 0.03443533927202225\n",
            "Epoch 5, Loss: 0.021336844190955162\n",
            "Epoch 6, Loss: 0.018526969477534294\n",
            "Epoch 7, Loss: 0.04886858910322189\n",
            "Epoch 8, Loss: 0.024182312190532684\n",
            "Epoch 9, Loss: 0.017775293439626694\n",
            "Epoch 10, Loss: 0.03204552084207535\n"
          ]
        }
      ],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "        return F_loss.mean()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UNet(in_channels=3, out_channels=1).to(device)\n",
        "criterion = FocalLoss()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owv4rFcTz21C"
      },
      "source": [
        "# Test IoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bDoaewSKcsYe",
        "outputId": "aeb2c65b-fb25-49ed-c504-2f600822f705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average IoU on the test set: 0.6501545310020447\n"
          ]
        }
      ],
      "source": [
        "def calculate_iou(preds, labels):\n",
        "    smooth = 1e-6\n",
        "    preds = torch.sigmoid(preds)\n",
        "    preds = (preds > 0.5).float()\n",
        "    preds = preds.int()\n",
        "    labels = labels.int()\n",
        "\n",
        "    intersection = (preds & labels).sum((1, 2))\n",
        "    union = (preds | labels).sum((1, 2))\n",
        "\n",
        "    iou = (intersection + smooth) / (union + smooth)\n",
        "    return iou.mean()\n",
        "\n",
        "model.eval()\n",
        "total_iou = 0\n",
        "count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, masks in test_loader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        iou = calculate_iou(outputs, masks)\n",
        "        total_iou += iou\n",
        "        count += 1\n",
        "\n",
        "average_iou = total_iou / count\n",
        "print(f'Average IoU on the test set: {average_iou.item()}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}